import os
import pandas as pd
import torch
from transformers import pipeline
from tqdm import tqdm
import warnings
import logging
from concurrent.futures import ThreadPoolExecutor
import numpy as np
from dotenv import load_dotenv
from datetime import datetime
import matplotlib.pyplot as plt

# üîß Chargement des variables d'environnement
load_dotenv()
DATA_EXPORTS = os.getenv("DATA_EXPORTS")
DATA_PROCESSED = os.getenv("DATA_PROCESSED")
DATA_REPORT = os.getenv("DATA_REPORT")  # nouveau pour le dossier report
LOG_DIR = os.getenv("LOG_DIR")

# üìÅ Fichiers d'entr√©e et sortie
INPUT_FILE = os.path.join(DATA_EXPORTS, "mongo_trustpilot_avis_trustpilot.csv")
OUTPUT_FILE = os.path.join(DATA_PROCESSED, "export_sentiment_analysis.csv")
STATS_FILE = os.path.join(DATA_PROCESSED, "stats_sentiment_analysis.csv")
REPORT_PNG = os.path.join(DATA_REPORT, "report_sentiment_analysis.png")

# üìö Log automatique dans le bon r√©pertoire
log_filename = os.path.join(LOG_DIR, f"bert_sentiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

# Cr√©ation des dossiers si absents
os.makedirs(DATA_PROCESSED, exist_ok=True)
os.makedirs(DATA_REPORT, exist_ok=True)

# üîÑ Renforcement des n√©gations
def reinforce_negations(text):
    if pd.isna(text) or not isinstance(text, str):
        return text
    negations = ['pas', 'plus', 'jamais', 'rien', 'aucun', 'ni', 'nulle part', 'ne', 'non']
    tokens = text.split()
    result = []
    i = 0
    while i < len(tokens):
        word = tokens[i].lower()
        if word in negations:
            result.append('[NEG]')
            result.append(tokens[i])
            j = 1
            while j <= 3 and (i + j) < len(tokens):
                result.append(tokens[i + j])
                j += 1
            result.append('[/NEG]')
            i += j
        else:
            result.append(tokens[i])
            i += 1
    return ' '.join(result)

# ü§ñ Analyseur BERT
class SentimentAnalyzer:
    def __init__(self):
        self.device = -1  # CPU
        self.model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
        self.batch_size = 4
        self.max_length = 128
        logger.info("ü§ñ Chargement du mod√®le BERT pour l'analyse de sentiment...")
        try:
            self.pipeline = pipeline(
                "sentiment-analysis",
                model=self.model_name,
                device=self.device,
                truncation=True
            )
            logger.info("‚úÖ Mod√®le BERT charg√© avec succ√®s !")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le : {str(e)}")
            raise

    def analyze_sentiment(self, text):
        try:
            if not text or pd.isna(text):
                return None, None
            result = self.pipeline(text[:self.max_length])
            return result[0]['label'], result[0]['score']
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur sur le texte : {text[:50]}... - {str(e)}")
            return None, None

    def analyze_batch(self, texts):
        with ThreadPoolExecutor() as executor:
            results = list(tqdm(
                executor.map(self.analyze_sentiment, texts),
                total=len(texts),
                desc="üß† Analyse des sentiments"
            ))
        return results

    def map_label_to_score(self, label):
        mapping = {
            '1 star': 1,
            '2 stars': 2,
            '3 stars': 3,
            '4 stars': 4,
            '5 stars': 5
        }
        return mapping.get(label, None)

# üì• Chargement des donn√©es
def load_data(filepath):
    logger.info(f"üìÇ Chargement des donn√©es depuis {filepath}...")
    try:
        df = pd.read_csv(filepath)
        logger.info(f"‚úÖ Donn√©es charg√©es : {len(df)} avis.")
        return df
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement : {str(e)}")
        raise

# üßπ Pr√©traitement
def preprocess_data(df):
    logger.info("üßπ Pr√©traitement des donn√©es...")
    df = df.drop_duplicates(subset=['commentaire'])
    df['commentaire'] = df['commentaire'].str.strip()
    df['commentaire'] = df['commentaire'].replace('', np.nan)
    df = df[df['commentaire'].notna()]
    logger.info("üîÑ Renforcement des n√©gations...")
    df['commentaire'] = df['commentaire'].apply(reinforce_negations)
    logger.info(f"üìä {len(df)} avis apr√®s nettoyage.")
    return df

# üìä G√©n√©ration du graphique de distribution
def plot_sentiment_distribution(stats_df):
    plt.figure(figsize=(8, 5))
    bars = plt.bar(stats_df["sentiment_note"], stats_df["count"], color='skyblue', edgecolor='black')
    plt.title("R√©partition des avis par note de sentiment (1 √† 5)", fontsize=14)
    plt.xlabel("Note de sentiment (1 = tr√®s n√©gatif, 5 = tr√®s positif)")
    plt.ylabel("Nombre d'avis")
    plt.xticks(stats_df["sentiment_note"])

    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2.0, yval + max(stats_df["count"]) * 0.01, f"{yval}", ha='center', va='bottom', fontsize=10)

    plt.tight_layout()
    plt.savefig(REPORT_PNG)
    logger.info(f"üìà Graphique sauvegard√© dans {REPORT_PNG}")
    plt.close()

# üöÄ Programme principal
def main():
    try:
        df = load_data(INPUT_FILE)
        df = preprocess_data(df)

        analyzer = SentimentAnalyzer()
        texts = df['commentaire'].tolist()
        results = analyzer.analyze_batch(texts)

        labels, scores = zip(*results)
        df['sentiment_label'] = labels
        df['sentiment_score'] = scores
        df['sentiment_note'] = df['sentiment_label'].apply(analyzer.map_label_to_score)

        df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
        logger.info(f"üíæ R√©sultats sauvegard√©s dans {OUTPUT_FILE}")
        print(f"\nüíæ Fichier final : {OUTPUT_FILE} avec {len(df)} lignes")

        stats = df['sentiment_note'].value_counts().sort_index()
        logger.info("üìà Statistiques des notes de sentiment :")
        logger.info(stats)

        stats_df = stats.reset_index()
        stats_df.columns = ['sentiment_note', 'count']
        stats_df.to_csv(STATS_FILE, index=False, encoding='utf-8-sig')
        logger.info(f"üìä Statistiques sauvegard√©es dans {STATS_FILE}")
        print("\nüìä R√©partition des sentiments :")
        print(stats_df.to_string(index=False))

        # G√©n√©ration du PNG
        plot_sentiment_distribution(stats_df)

    except Exception as e:
        logger.error(f"‚ùå Erreur dans le processus principal : {str(e)}")

if __name__ == "__main__":
    main()
