{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a029930",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186130b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb2d56",
   "metadata": {},
   "source": [
    "=======================\n",
    "Chargement .env\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca96b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BASE_DIR = os.getenv(\"BASE_DIR\")\n",
    "if not BASE_DIR:\n",
    "    print(\"‚ùå ERREUR : BASE_DIR non d√©fini dans .env\")\n",
    "    exit(1)\n",
    "\n",
    "DATA_PROCESSED = os.getenv(\"DATA_PROCESSED\", f\"{BASE_DIR}/data/processed\")\n",
    "DATA_REPORT = os.getenv(\"DATA_REPORT\", f\"{BASE_DIR}/data/report\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b99b1",
   "metadata": {},
   "source": [
    "Entr√©e / Sorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ad156",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = os.path.join(DATA_PROCESSED, \"export_clean_data.csv\")\n",
    "OUTPUT_CSV = os.path.join(DATA_PROCESSED, \"export_preprocess_clean_avis.csv\")\n",
    "OUTPUT_STATS = os.path.join(DATA_PROCESSED, \"stats_preprocess_clean_avis.csv\")\n",
    "OUTPUT_WORDCLOUD = os.path.join(DATA_REPORT, \"report_preprocess_clean_avis_word_cloud.png\")\n",
    "OUTPUT_SENTIMENT_HIST = os.path.join(DATA_REPORT, \"report_preprocess_clean_avis_sentiment_hist.png\")\n",
    "OUTPUT_LDA_IMG = os.path.join(DATA_REPORT, \"report_preprocess_clean_avis_lda.png\")\n",
    "\n",
    "os.makedirs(DATA_PROCESSED, exist_ok=True)\n",
    "os.makedirs(DATA_REPORT, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56ec0c",
   "metadata": {},
   "source": [
    "=======================\n",
    "Stopwords & spaCy\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb45c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "stop_fr = set(stopwords.words('french'))\n",
    "stop_en = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281a4bf",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Mots a bannir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_STOPWORDS = {\n",
    "    'vinted', 'temu', 'chronopost', 'tesla', 'amazon', 'ub√©r', 'uber', 'ubereats',\n",
    "    '√™tre', 'avoir', 'faire', 'aller', 'venir', 'tr√®s', 'jai', 'cest', 'cette',\n",
    "    'ce', 'ces', 'tout', 'tous', 'toute', 'toutes', 'comme', 'alors', 'alor', 'alorsque',\n",
    "    'du', 'du coup', 'donc', 'car', 'parce', 'parce que', 'puis', 'ensuite', 'apr√®s',\n",
    "    'un', 'une', 'des', 'le', 'la', 'les', 'au', 'aux', 'de', 'du', \"d'\",\n",
    "    'mon', 'ton', 'son', 'ma', 'ta', 'sa', 'mes', 'tes', 'ses',\n",
    "    'leur', 'leurs', 'notre', 'votre', 'nos', 'vos',\n",
    "    'en', 'y', 'avec', 'sans', 'plus', 'sur', 'dire',\n",
    "    'je', 'tu', 'il', 'elle', 'on', 'nous', 'vous', 'ils', 'elles',\n",
    "    'ce', 'cet', 'cette', 'ces', 'tout', 'tous', 'toute', 'toutes',\n",
    "    'comme', 'alors', 'alor', 'alorsque', 'du', 'du coup', 'donc',\n",
    "    'car', 'parce', 'parce que', 'puis', 'ensuite', 'apr√®s',\n",
    "    'un', 'une', 'des', 'le', 'la', 'les', 'au', 'aux', 'de', 'du', \"d'\",\n",
    "    'mon', 'ton', 'son', 'ma', 'ta', 'sa', 'mes', 'tes', 'ses',\n",
    "    'leur', 'leurs', 'notre', 'votre', 'nos', 'vos',\n",
    "    'en', 'y', 'avec', 'sans', 'plus', 'sur', 'sous', 'dans', 'chez', 'etc'\n",
    "}\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_sm\")\n",
    "    print(\"‚úÖ spaCy charg√© avec le mod√®le fran√ßais 'fr_core_news_sm'\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå spaCy non charg√© :\", e)\n",
    "    exit(1)\n",
    "\n",
    "all_stopwords = stop_fr.union(stop_en).union(CUSTOM_STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea613618",
   "metadata": {},
   "source": [
    "Stopwords sp√©cifiques au wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7505b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WC_STOPWORDS = set(STOPWORDS)\n",
    "WC_STOPWORDS |= CUSTOM_STOPWORDS\n",
    "WC_STOPWORDS.update([\"neg\", \"cest\", \"cette\", \"ce\", \"ces\", \"tout\", \"tous\", \"toute\", \"toutes\",\n",
    "                    \"comme\", \"alors\", \"alor\", \"alorsque\", \"du\", \"du coup\", \"donc\",\n",
    "                    \"car\", \"parce\", \"parce que\", \"puis\", \"ensuite\", \"apr√®s\",\n",
    "                    \"un\", \"une\", \"des\", \"le\", \"la\", \"les\", \"au\", \"aux\", \"de\", \"du\", \"d'\",\n",
    "                    \"mon\", \"ton\", \"son\", \"ma\", \"ta\", \"sa\", \"mes\", \"tes\", \"ses\",\n",
    "                    \"leur\", \"leurs\", \"notre\", \"votre\", \"nos\", \"vos\",\n",
    "                    \"en\", \"y\", \"avec\", \"sans\", \"plus\", \"sur\", \"sous\", \"dans\", \"chez\", \"etc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6e411",
   "metadata": {},
   "source": [
    "=======================\n",
    "Pr√©traitement + n√©gation\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_negation(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884be279",
   "metadata": {},
   "source": [
    "    # Nettoyage de base + retrait des balises [NEG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\[/?neg\\]\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "    negation_words = {\"ne\", \"pas\", \"plus\", \"jamais\", \"rien\", \"personne\", \"aucun\", \"ni\", \"non\"}  # <-- ajout de 'non'\n",
    "    negate_next = False\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_space or token.is_punct:\n",
    "            continue\n",
    "\n",
    "        lemma = token.lemma_.strip().lower()\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        if lemma in negation_words:\n",
    "            negate_next = True\n",
    "            continue\n",
    "\n",
    "        if negate_next:\n",
    "            if lemma not in all_stopwords and len(lemma) > 2:\n",
    "                tokens.append(f\"neg_{lemma}\")\n",
    "            negate_next = False\n",
    "            continue\n",
    "\n",
    "        if lemma == \"neg\":\n",
    "            continue\n",
    "\n",
    "        if lemma not in all_stopwords and len(lemma) > 2:\n",
    "            tokens.append(lemma)\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a57a02",
   "metadata": {},
   "source": [
    "=======================\n",
    "Utilitaires\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(text, top_n=20):\n",
    "    words = [w for w in text.split() if w not in CUSTOM_STOPWORDS and w != \"neg\"]\n",
    "    counter = Counter(words)\n",
    "    top_words = counter.most_common(top_n)\n",
    "    print(\"Top 20 mots les plus fr√©quents\")\n",
    "    for w, c in top_words:\n",
    "        print(f\"{w:<20} : {c}\")\n",
    "    return counter\n",
    "\n",
    "def generate_wordcloud(text):\n",
    "    wc = WordCloud(\n",
    "        width=800, height=400, background_color='white',\n",
    "        max_words=200, colormap='viridis',\n",
    "        stopwords=WC_STOPWORDS\n",
    "    ).generate(text)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_WORDCLOUD)\n",
    "    plt.close()\n",
    "\n",
    "def generate_lda(texts, num_topics=5):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c26121",
   "metadata": {},
   "source": [
    "    # Filtrage s√©curit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cleaned = [\" \".join([w for w in t.split() if w not in CUSTOM_STOPWORDS and w != \"neg\"]) for t in texts]\n",
    "\n",
    "    tokenized = [t.split() for t in cleaned]\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    corpus = [dictionary.doc2bow(t) for t in tokenized]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
    "    topics = lda_model.show_topics(num_topics=num_topics, num_words=6, formatted=False)\n",
    "\n",
    "    print(\"Th√®mes d√©tect√©s (LDA) :\")\n",
    "    for i, topic in topics:\n",
    "        print(f\" - Th√®me {i+1} : {', '.join([w for w, _ in topic])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ca666",
   "metadata": {},
   "source": [
    "    # Visualisation tr√®s simple : barres par th√®me (somme des poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803372ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, topic in topics:\n",
    "        words = [w for w, _ in topic]\n",
    "        weights = [w_ for _, w_ in topic]\n",
    "        plt.barh([f\"Th√®me {i+1} : {w}\" for w in words], weights)\n",
    "    plt.title(\"Th√®mes LDA - top mots\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_LDA_IMG)\n",
    "    plt.close()\n",
    "\n",
    "    return [{\"theme\": i+1, \"words\": [w for w, _ in topic]} for i, topic in topics]\n",
    "\n",
    "def sentiment_stats(texts):\n",
    "    polarities = [TextBlob(t).sentiment.polarity for t in texts]\n",
    "    pos = sum(p > 0.2 for p in polarities)\n",
    "    neu = sum(-0.2 <= p <= 0.2 for p in polarities)\n",
    "    neg = sum(p < -0.2 for p in polarities)\n",
    "    total = len(polarities)\n",
    "    print(\"Distribution sentiments TextBlob\")\n",
    "    print(f\" - Positifs : {pos} ({pos/total:.1%})\")\n",
    "    print(f\" - Neutres  : {neu} ({neu/total:.1%})\")\n",
    "    print(f\" - N√©gatifs : {neg} ({neg/total:.1%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516a5ec",
   "metadata": {},
   "source": [
    "    # Histogramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(polarities, bins=20, color='cornflowerblue', edgecolor='black')\n",
    "    plt.title(\"Distribution des sentiments\")\n",
    "    plt.xlabel(\"Polarit√©\")\n",
    "    plt.ylabel(\"Nombre d'avis\")\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_SENTIMENT_HIST)\n",
    "    plt.close()\n",
    "\n",
    "    return {\"positif\": pos, \"neutre\": neu, \"negatif\": neg, \"total\": total}\n",
    "\n",
    "def save_report(stats, top_words_counter, lda_topics):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e8181",
   "metadata": {},
   "source": [
    "    # Filtrage s√©curit√© pour le CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8569f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    top20_filtered = [(w, c) for w, c in top_words_counter.items() if w not in CUSTOM_STOPWORDS and w != \"neg\"]\n",
    "    top20_filtered = sorted(top20_filtered, key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    df_stats = pd.DataFrame({\n",
    "        \"nb_avis\": [stats[\"total\"]],\n",
    "        \"sentiments_positif\": [stats[\"positif\"]],\n",
    "        \"sentiments_neutre\": [stats[\"neutre\"]],\n",
    "        \"sentiments_negatif\": [stats[\"negatif\"]],\n",
    "        \"top20_mots\": [\"; \".join([f\"{w}:{c}\" for w, c in top20_filtered])],\n",
    "        \"lda_resume\": [\" | \".join([f\"Th√®me {t['theme']} : {', '.join(t['words'])}\" for t in lda_topics])]\n",
    "    })\n",
    "    df_stats.to_csv(OUTPUT_STATS, index=False)\n",
    "    print(f\"‚úÖ Statistiques CSV sauvegard√©es : {OUTPUT_STATS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2cf89",
   "metadata": {},
   "source": [
    "=======================\n",
    "Main\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d263c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"üì• Lecture du fichier :\", INPUT_CSV)\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    print(f\"{len(df)} avis charg√©s\")\n",
    "\n",
    "    print(\"üßº Nettoyage & lemmatisation + gestion n√©gation (patch custom stopwords)...\")\n",
    "    df['commentaire_preprocessed'] = df['commentaire'].apply(preprocess_text_with_negation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0c75b",
   "metadata": {},
   "source": [
    "    # Filtrage s√©curit√© final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df['commentaire_preprocessed'] = df['commentaire_preprocessed'].apply(\n",
    "        lambda t: \" \".join([w for w in t.split() if w not in CUSTOM_STOPWORDS and w != \"neg\"])\n",
    "    )\n",
    "\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"‚úÖ Fichier nettoy√© sauvegard√© : {OUTPUT_CSV}\")\n",
    "\n",
    "    series_clean = df['commentaire_preprocessed'].dropna()\n",
    "\n",
    "    all_text = \" \".join(series_clean)\n",
    "    top_words_counter = print_top_words(all_text)\n",
    "\n",
    "    generate_wordcloud(all_text)\n",
    "    print(f\"‚úÖ Wordcloud sauvegard√© : {OUTPUT_WORDCLOUD}\")\n",
    "\n",
    "    lda_topics = generate_lda(series_clean)\n",
    "    print(f\"‚úÖ Graphique LDA sauvegard√© : {OUTPUT_LDA_IMG}\")\n",
    "\n",
    "    stats = sentiment_stats(series_clean)\n",
    "    print(f\"‚úÖ Histogramme des sentiments sauvegard√© : {OUTPUT_SENTIMENT_HIST}\")\n",
    "\n",
    "    save_report(stats, top_words_counter, lda_topics)\n",
    "\n",
    "    print(\"‚úÖ Analyse termin√©e\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
