{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f7b39",
   "metadata": {},
   "source": [
    "üîπ Chargement des variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68064fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DATA_PROCESSED = os.getenv(\"DATA_PROCESSED\")\n",
    "DATA_MODEL = os.getenv(\"DATA_MODEL\")\n",
    "DATA_REPORT = os.getenv(\"DATA_REPORT\")\n",
    "os.makedirs(DATA_MODEL, exist_ok=True)\n",
    "os.makedirs(DATA_REPORT, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66f761",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DATA_PROCESSED, \"export_preprocess_clean_avis.csv\")\n",
    "print(\"\\nüì• Chargement des donn√©es...\")\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"üìÑ Colonnes du fichier : {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f34a1",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c13cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['commentaire'])\n",
    "df = df[df['commentaire'].str.strip().astype(bool)]\n",
    "print(f\"‚úÖ Nombre de lignes apr√®s nettoyage : {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373ae77",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Fonction avanc√©e pour g√©rer la n√©gation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_negation(text, window=3):\n",
    "    negation_words = {\"ne\", \"pas\", \"plus\", \"jamais\", \"rien\", \"aucun\", \"sans\", \"nul\"}\n",
    "    punctuation = {\".\", \",\", \";\", \":\", \"!\", \"?\"}\n",
    "    stop_words = {\"mais\", \"et\", \"ou\", \"donc\", \"or\", \"ni\", \"car\"}\n",
    "    tokens = text.split()\n",
    "    new_tokens = []\n",
    "    neg_countdown = 0\n",
    "    for tok in tokens:\n",
    "        tok_lower = tok.lower()\n",
    "        if tok_lower in negation_words:\n",
    "            neg_countdown = window\n",
    "            new_tokens.append(tok)\n",
    "        elif neg_countdown > 0:\n",
    "            if tok_lower in punctuation or tok_lower in stop_words:\n",
    "                neg_countdown = 0\n",
    "                new_tokens.append(tok)\n",
    "            else:\n",
    "                new_tokens.append(\"NOT_\" + tok)\n",
    "                neg_countdown -= 1\n",
    "        else:\n",
    "            new_tokens.append(tok)\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "df['commentaire_preprocessed'] = df['commentaire'].apply(mark_negation)\n",
    "X = df['commentaire_preprocessed']\n",
    "y_notes = df['note_commentaire']\n",
    "\n",
    "def map_sentiment(note):\n",
    "    if note == 1: return 'negatif'\n",
    "    elif note == 5: return 'positif'\n",
    "    else: return 'neutre'\n",
    "\n",
    "y_sentiment = y_notes.apply(map_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbcdbb",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úçÔ∏è Vectorisation TF-IDF avec bigrammes...\")\n",
    "tfidf = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "X_vect = tfidf.fit_transform(X)\n",
    "joblib.dump(tfidf, os.path.join(DATA_MODEL, \"tfidf_vectorizer_dual.pkl\"))\n",
    "print(f\"üíæ TF-IDF vectorizer sauvegard√© dans {DATA_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3688fa8",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Split train/test stratifi√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2293d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in sss.split(X_vect, y_sentiment):\n",
    "    X_train, X_test = X_vect[train_index], X_vect[test_index]\n",
    "    y_train_sent, y_test_sent = y_sentiment.iloc[train_index], y_sentiment.iloc[test_index]\n",
    "    y_train_note, y_test_note = y_notes.iloc[train_index], y_notes.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d596e4f",
   "metadata": {},
   "source": [
    "5.1Ô∏è‚É£ R√©-√©chantillonnage SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ac092",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà Application du r√©-√©chantillonnage pour √©quilibrer les classes...\")\n",
    "smote_sent = SMOTE(random_state=42)\n",
    "X_train_sent, y_train_sent = smote_sent.fit_resample(X_train, y_train_sent)\n",
    "smote_note = SMOTE(random_state=42)\n",
    "X_train_note, y_train_note = smote_note.fit_resample(X_train, y_train_note)\n",
    "print(f\"‚úÖ Classes √©quilibr√©es pour sentiment et notes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ebc7f",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ Fonction pour cr√©er le mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73968567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(name):\n",
    "    if name == \"LogisticRegression\": return LogisticRegression(max_iter=1000, class_weight=None)\n",
    "    elif name == \"LinearSVC\": return LinearSVC(class_weight=None)\n",
    "    elif name == \"RandomForest\": return RandomForestClassifier(n_estimators=100, class_weight=None, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345832e5",
   "metadata": {},
   "source": [
    "7Ô∏è‚É£ D√©finition des t√¢ches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"sentiment\": (X_train_sent, y_train_sent, X_test, y_test_sent, ['negatif','neutre','positif']),\n",
    "    \"note\": (X_train_note, y_train_note, X_test, y_test_note, [1,2,3,4,5])\n",
    "}\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb65735",
   "metadata": {},
   "source": [
    "Fonction graphique pour top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_features(model, feature_names, task, model_name, n_features=20):\n",
    "    try:\n",
    "        if hasattr(model,'coef_'):\n",
    "            coef = model.coef_\n",
    "            coef = coef.flatten() if coef.shape[0]==1 else np.mean(np.abs(coef), axis=0)\n",
    "        elif hasattr(model,'feature_importances_'):\n",
    "            coef = model.feature_importances_\n",
    "        else:\n",
    "            return\n",
    "        indices = np.argsort(np.abs(coef))[-n_features:]\n",
    "        top_words = feature_names[indices]\n",
    "        top_values = coef[indices]\n",
    "        plt.figure(figsize=(10,6))\n",
    "        colors = ['green' if v>0 else 'red' for v in top_values] if hasattr(model,'coef_') else 'blue'\n",
    "        plt.barh(top_words, top_values, color=colors)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        filename = f\"report_preprocess_top{n_features}_{model_name.lower()}_{task}.png\"\n",
    "        plt.savefig(os.path.join(DATA_REPORT, filename))\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur top features {model_name} ({task}): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ea126",
   "metadata": {},
   "source": [
    "8Ô∏è‚É£ Entra√Ænement et √©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a21c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, (X_tr, y_tr, X_te, y_te, labels) in tasks.items():\n",
    "    print(f\"\\n===== üîπ T√¢che : {task.upper()} =====\")\n",
    "    for name in [\"LogisticRegression\",\"LinearSVC\",\"RandomForest\"]:\n",
    "        print(f\"\\n‚ö° Entra√Ænement du mod√®le {name}...\")\n",
    "        model = get_model(name)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_te)\n",
    "        acc = accuracy_score(y_te, y_pred)\n",
    "        f1 = f1_score(y_te, y_pred, average='macro', zero_division=0)\n",
    "        print(f\"üìä Accuracy: {acc*100:.2f}% | F1-score macro: {f1:.4f}\")\n",
    "        print(classification_report(y_te, y_pred, zero_division=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753eb34f",
   "metadata": {},
   "source": [
    "        # Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        cm = confusion_matrix(y_te, y_pred, labels=labels)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f\"Matrice de confusion - {name} ({task})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(DATA_REPORT,f\"report_preprocess_confusion_{name.lower()}_{task}.png\"))\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d10c6",
   "metadata": {},
   "source": [
    "        # Top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "        feature_names = np.array(tfidf.get_feature_names_out())\n",
    "        plot_top_features(model, feature_names, task, name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8229a5",
   "metadata": {},
   "source": [
    "        # Sauvegarde du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb955609",
   "metadata": {},
   "outputs": [],
   "source": [
    "        joblib.dump(model, os.path.join(DATA_MODEL,f\"{name.lower()}_{task}.pkl\"))\n",
    "        results.append({\"T√¢che\":task,\"Mod√®le\":name,\"Accuracy\":round(acc,4),\"F1_score_macro\":round(f1,4)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c297f8b",
   "metadata": {},
   "source": [
    "9Ô∏è‚É£ R√©sum√© comparatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e16cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nüìã R√©sum√© comparatif des mod√®les :\")\n",
    "print(df_results)\n",
    "df_results.to_csv(os.path.join(DATA_PROCESSED,\"resultats_modeles.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab603b9",
   "metadata": {},
   "source": [
    "üîü Tests manuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e8a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = {\n",
    "    \"positif\":[\"Super service, je suis tr√®s satisfait !\",\"Livraison rapide et produit conforme\",\"Exp√©rience excellente du d√©but √† la fin\"],\n",
    "    \"neutre\":[\"C'√©tait correct, sans plus\",\"Pas de probl√®me mais rien d'extraordinaire\",\"Service moyen, livraison standard\"],\n",
    "    \"negatif\":[\"Service catastrophique, √† fuir\",\"Je ne suis pas content du tout\",\"Produit d√©fectueux et aucune r√©ponse du SAV\"]\n",
    "}\n",
    "print(\"\\nüß™ Tests manuels :\")\n",
    "for cat,samples in phrases.items():\n",
    "    for phrase in samples:\n",
    "        vec = tfidf.transform([mark_negation(phrase)])\n",
    "        print(f\"\\nüí¨ Phrase ({cat}): {phrase}\")\n",
    "        for task in [\"sentiment\",\"note\"]:\n",
    "            print(f\"üîπ Pr√©dictions {task}:\", end=\" \")\n",
    "            for name in [\"LogisticRegression\",\"LinearSVC\",\"RandomForest\"]:\n",
    "                model_path = os.path.join(DATA_MODEL,f\"{name.lower()}_{task}.pkl\")\n",
    "                if os.path.exists(model_path):\n",
    "                    mdl = joblib.load(model_path)\n",
    "                    pred = mdl.predict(vec)[0]\n",
    "                    print(f\"{name}={pred}\", end=\" | \")\n",
    "            print()\n",
    "\n",
    "print(\"\\n‚úÖ Script complet termin√©.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
